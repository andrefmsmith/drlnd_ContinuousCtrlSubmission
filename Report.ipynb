{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control in Reacher with DDPG - Report\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is my submission report for Project 2 (Continuous Control) of Udacity's Deep Reinforcement Learning Nanodegree. In this project, we train an agent in a Unity environment to control a double-jointed arm and reach for a moving target. This is a task with a 33-dimensional continuous state space and 4-dimensional action space. I chose to implement DDPG in order to solve the **single-agent** version of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background & overview of Actor-Critic and DDPG\n",
    "In [Project 1](https://github.com/andrefmsmith/drlnd_NavigationSubmission/blob/master/Report.ipynb) we saw how deep neural networks could be used as function approximators for arriving at an estimate of Q, the action-value function. In particular, we implemented a [Deep Q Network (DQN)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). DQNs learn to evaluate policies by minimizing a loss between predicted and target Q values. DQNs were an important advance in DRL research since they improved upon two big problems of using deep neural networks for action-value function approximation, which meant this approach was previously too unstable: i) the **correlation between trajectories** experienced by the agent (by using Replay Buffers), and  ii) the problem of **moving targets** when using the same network to estimate a value and setting a target for learning (by using fixed Q targets). Deep value-based approaches mainly work with discrete action spaces, since the policy is arrived by using the argmax operator over action values. This makes them unwieldy for high-dimensional and/or continuous action spaces.  \n",
    "\n",
    "In subsequent lectures, we saw how it was actually possible to maximise the optimization objective of returns over time directly, by using **policy gradient** approaches such as [REINFORCE](https://github.com/andrefmsmith/amsRL_openAIgym/blob/master/Pong/CodeBlog_PongFromPixels_2.0.ipynb). In this and more advanced policy gradient approaches, agents learn policies directly, without the need to estimate value functions, by performing gradient ascent on the expected return function and tweaking parameters to produce actions that lead to high returns with increased probability. Policy gradient approaches are useful because policies can be any learnable function, including continuous and stochastic action spaces. Moreover, they can have better convergence properties, since action probabilities change smoothly as a function of learned parameters as opposed to aggressively due to the use of argmax in value-based approaches. However, policy gradient approaches have high variance because we use full Monte-Carlo returns to calculate gradients, compounding the randomness and variability of trajectories under the same policy.  \n",
    "\n",
    "**Actor-Critic** methods are an attempt to get the best of both worlds by using an estimate of the action-value or advantage function instead of actual return to reduce the variance of policy gradients. In AC approaches we use two neural networks: an **Actor** learns the policy and a **Critic** learns a value function. The value function estimated by the critic is used to train the Actor instead of the total returns, which reduces variance and can lead to faster learning.  \n",
    "\n",
    "**[DDPG](https://arxiv.org/abs/1509.02971)** is an Actor-Critic method (according to some) which, similar to DQN, uses a replay buffer to train an action-value function and target networks to stabilize training. Differently to DQN, it learns a policy directly with an Actor network, which approximates the optimal action and can learn a deterministic policy in continuous action spaces. The subtle difference between DQN and DDPG is that whereas the former uses the target Q-function for retrieving the greedy action using argmax, DDPG uses a target deterministic policy function that is *trained to approximate* that greedy action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent & Methods\n",
    "______________________\n",
    "### Algorithm overview\n",
    "1. Define an **online** and a **target** instance for the **Actor** and **Critic** networks. Actor networks are *states in, actions out* whereas Critic networks are *states in, single action value out*. However, because they estimate the Q value for a given action, Critic networks need to be *fed an action as well as the state*. This happens at their first hidden layer.\n",
    "2. At every timestep, feed the state into the online Actor network in order to return the greedy action according to the current policy.\n",
    "    - To enable exploration, add scaled noise from a normal distribution to this output and clip the result to the boundaries of the action space. As training proceeds, scale this noise down.\n",
    "3. Store the timestep's state, performed action, reward, next state and terminal flag in the agent's replay buffer.\n",
    "4. If the number of timesteps stored has completed a batch, and if a pre-defined number of timesteps have passed since the last update epoch, run an optimization step.\n",
    "5. Optimization step:\n",
    "    - Retrieve k (batch size) experience tuples from the replay buffer;\n",
    "    - Calculate targets:\n",
    "        - Query the **Actor target** network for the best actions to perform in the next states\n",
    "        - Feed these actions and states to the **Critic target** network thus obtaining their Q value\n",
    "        - Compute Q targets by adding observed rewards and discounting the Q values of the next states (computed above)\n",
    "    - Compute the expected Q value for current states and actions by querying the **Critic online** network\n",
    "    - Compute **loss for the online critic network** by using mean square error between expected and target Q value\n",
    "    - Compute the current policy's best action by querying the **online Actor** network\n",
    "    - Compute **loss for the online actor network** by evaluating the value of the current policy's best action with the online critic network\n",
    "        - Use the negative of the loss, so that optimization **maximises** the value of actions outputted by the policy network\n",
    "6. Update the target networks by blending the online and target network weights according to a tau-parameterised weighted average, thus ensuring the target network is changing at a much slower rate than the online and producing stable targets.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor and Critic network architectures\n",
    "\n",
    "Here we define classes for the Actor and Critic architectures, which we'll call on later when setting up our Agent for DDPG. **Actor** is a *'state-in action-out'* network with one hidden layer.\n",
    "```Python\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(400, 300), activ_in = F.relu, activ_out = torch.tanh):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        self.activ_out = activ_out\n",
    "        \n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0], hu[1])\n",
    "        self.output_layer = nn.Linear(hu[-1], action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = self.activ_in(self.input_layer(x))\n",
    "        x = self.activ_in(self.hl1(x))\n",
    "        return self.activ_out(self.output_layer(x)) \n",
    "        ```\n",
    "The input layer size matches the state size and the output layer size matches the action space. In our forward method, we use ReLu for inner layer activations and tanH for output because the boundaries for our action space are [-1, 1]. If they were higher, we would need to scale them appropriately. I tried deeper networks but training was significantly slower.  \n",
    "\n",
    "\n",
    "**Critic** is a *'state-in Q value-out'* network with one hidden layer. We want this network to learn how to approximate the Q value of a specific action and to achieve this, it needs to know both the state and the action.\n",
    "```Python\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(400, 300), activ_in = F.relu):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        \n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0]+action_size, hu[1])        \n",
    "        self.output_layer = nn.Linear(hu[-1], 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = state\n",
    "        u = action\n",
    "        \n",
    "        x = self.activ_in(self.input_layer(x))\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        x = self.activ_in(self.hl1(x))\n",
    "        return self.output_layer(x)```\n",
    "The input layer matches state size. However, at the first hidden layer we add a number of units equal to the action size, so that during forward propagation we can feed this network an action so that it can use state and action information to approximate a Q value in its output. The action size is small enough (4-dimensional) that it requires less processing and transformation thatn the state. I imagine with a higher action size we may want to feed it to the input layer. As we wish the Critic to learn how to approximate the action value of a single action, the output size is 1 and requires no activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent's attributes\n",
    "In addition to state size and action size I set up my agent to take in a number of noise-related arguments, which we will use for exploratory actions later on:\n",
    "```Python\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, start_noise=SN, noise_decay=ND, noise_min=NM, add_noise=True):```\n",
    "We make use of our previously-defined Actor and Critic classes to create an online and a target network for each. We also set up two separate optimizers as we may want the learning rate for Actor and Critic to be different:\n",
    "```Python\n",
    "        ### Initialise actor online and target networks\n",
    "        self.actor_online = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_online.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        ### Initialise critic online and target networks\n",
    "        self.critic_online = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_online.parameters(), lr=LR_CRITIC)```\n",
    "\n",
    "In addition, we initialise the previously-mentioned noise parameters, a memory that instantiates an object of class ReplayBuffer (to be defined later) and an update attribute which will keep track of how many timesteps have occurred since we ran an optimization step:\n",
    "```Python\n",
    "        ### Noise parameters for exploration\n",
    "        self.noise_scale = start_noise\n",
    "        self.noise_decay = noise_decay\n",
    "        self.noise_min = noise_min\n",
    "        \n",
    "        self.add_noise = add_noise\n",
    "        \n",
    "        ### Replay buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        ### Keep track of timesteps since last training update\n",
    "        self.update = 0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent's methods\n",
    "\n",
    "**Generating noise for exploratory actions**  \n",
    "I chose a simple strategy for setting up an exploration schedule for the agent. I define a function that generates noise and updates the noise's scale over time by calling on the Agent's noise attributes which are defined at initialization:\n",
    "```Python\n",
    "    def generate_noise(self):\n",
    "        noise = np.random.normal(loc=0, scale=self.noise_scale, size=self.action_size)\n",
    "        self.noise_scale = max(self.noise_decay*self.noise_scale, self.noise_min)\n",
    "        return noise```\n",
    "The numpy method np.random.normal(loc, scale, size) returns a 'size' number of floats drawn from a normal distribution with mean 'loc' and standard deviation 'scale'. The action space in this task spans [-1, 1] so we want this normal distribution to be centered at 0 (loc=0). I use the Agent's noise_scale attribute as standard deviation and return a number of floats equal to action size. Alternatively, I could have returned a single random float to be added to each entry in the action vector. I felt the former seemed more robust, since it would build up independence in the outputs of the policy network for each entry in the action vector. After generating noise, the 'generate_noise' function progresses by updating the Agent's internal 'noise_scale' attribute according to an exponential schedule, with constant defined by agent attribute 'noise_decay'. A minimum amount of noise 'noise_min' is kept throughout, to ensure a baseline level of exploration. I feel this is also a form of regularization and ensuring representation robustness, in the sense that since random noise is always added to the agent's output, this helps attenuate parameter overfitting.  \n",
    "\n",
    "**Controlling the Agent's progression**  \n",
    "We need a method for controlling the agent's progression through time. The 'step' method stores fresh experiences in the replay buffer and triggers an optimization epoch if appropriate:\n",
    "```Python\n",
    "def step(self, state, action, reward, next_state, done, update_cycles=3):\n",
    "        # Commit experience to memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.update = (self.update +1)%update_every\n",
    "        \n",
    "        # Run optimisation 'update_cycles' times\n",
    "        c=0\n",
    "        if (self.update==0):\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                while c < update_cycles:\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, GAMMA)\n",
    "                    c+=1```\n",
    "Experience tuples are added to the replay buffer using it's method 'add' (to be defined later) and then the agent updates how many time steps have passed since it ran an optimization cycle. The amount of timesteps to wait before optimization is controlled by hyperparameter 'update_every'. If this number has been reached *and* a full batch of timesteps has elapsed (controlled by hyperparameter 'BATCH_SIZE'), the agent will sample experience tuples from memory (.sample method of replay buffer class, to be defined) and run an optimization sequence on them. This will happen inside a 'while' loop, so that the agent samples experiences and learns an 'update_cycles' number of times from them.  \n",
    "\n",
    "**Selecting actions and exploring**  \n",
    "The agent will use its online Actor network to output a greedy action:\n",
    "```Python\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_online.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_online(state).cpu().data.numpy()\n",
    "        self.actor_online.train()\n",
    "        if self.add_noise:\n",
    "            action += self.generate_noise()\n",
    "        return np.clip(action, -1, 1)```\n",
    "To ensure exploration, if we have set the Agent's 'add_noise' attribute to True, the 'act'function calls on 'generate_noise', and adds the output of this method to the Agent's action, updating also its noise schedule. The function return clips this action to the boundaries of the action space [-1, 1] in case they have been exceeded by the adding of noise.  \n",
    "\n",
    "**Updating the Agent**  \n",
    "Finally, we have a function for performing parameter updating. **First**, this function unpacks the experience tuples sampled from the replay buffer into individual tensors:\n",
    "```Python\n",
    "def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences```\n",
    "        \n",
    "**Second**, it updates the Critic network. To achieve this, it must set optimization targets by estimating returns from the current state, computing the loss between target and online estimates and backpropagate it to update parameters. We do this in the following steps:\n",
    "1. Query the target Actor network for the actions to be performed in the next states;\n",
    "2. Query the target Critic network for the estimated value of these future actions;\n",
    "3. Compute a learning target for the value of the current action by discounting future action values and adding to them observed rewards;\n",
    "4. Query the online Critic network for its estimate of value for the action;\n",
    "5. Compute the mean square error loss between the online Critic's expectation and the target Critic's learning target; and finally,\n",
    "6. Backpropagate and clip the gradient to stabilize learning.\n",
    "\n",
    "```Python\n",
    "        ### Update critic\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1-dones))\n",
    "        Q_expected = self.critic_online(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Backprop\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_online.parameters(), 1)\n",
    "        self.critic_optimizer.step()```  \n",
    "\n",
    "**Third**, we update the Actor network. This involves:\n",
    "1. Querying the online Actor for its predicted best action;\n",
    "2. Passing this action and the states to the online critic network to query its value;\n",
    "3. Using the *negative* of this as loss, given our goal is to maximise the value of actions chosen by the Actor network. Herein lines the key difference between policy gradient and AC approaches: in the former we obtained a Monte-Carlo estimate of the total returns and we maximised that function using gradient ascent. In the latter, we use a Critic network to estimate action value instead;\n",
    "4. We backpropagate this loss and once again clip the gradient to promote stability.\n",
    "\n",
    "```Python\n",
    "        ### Update actor\n",
    "        actions_pred = self.actor_online(states)\n",
    "        actor_loss = -self.critic_online(states, actions_pred).mean()\n",
    "        # Backprop\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor_online.parameters(), 1)\n",
    "        self.actor_optimizer.step()```  \n",
    "        \n",
    "**Finally** we must update our target networks:\n",
    "```Python\n",
    "    def soft_update(self, online_model, target_model, tau):\n",
    "        for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "            target_param.data.copy_(tau*online_param.data + (1.0-tau)*target_param.data)```\n",
    "Here we are not using fixed Q targets as such, given that we don't 'freeze' the target networks' parameters as we did for DQNs where target networks were frozen for 10,000 timesteps, before having their parameters overwritten by a copy of the online networks'. All we need to prevent moving targets is for the online networks approximating functions and the target networks generating learning targets to be in different states. We ensure that by using a hyperparameter 'tau' as a weighting factor for averaging the online and target networks' parameters. This effectively means that target networks lag behind their online counterparts at a very slow timescale (since we'll typically set tau to a 3-4 digit fractional number), but they are also being updated at the same rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ReplayBuffer class\n",
    "The ReplayBuffer has a 'memory' container for storing experience tuples and a series of methods to interact with them. \n",
    "```Python\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    ### Adds an experience tuple to memory\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)```\n",
    "The 'memory' container is a deque, meaning once 'maxlen' capacity is reached it behaves as latest one in, oldest one out. We also configure a series of class attributes. In particular 'experience' is set up as an namedtuple object, which makes retrieving and organising experiences in 'memory' easier.  \n",
    "\n",
    "We commit experiences to memory using the **add** method, which takes arguments for all the elements of an experience, formats them as a tuple as defined by the 'experience' namedtuple class then adds them at the latest slot of the 'memory' deque.  \n",
    "\n",
    "\n",
    "Finally, experiences are sampled from memory for learning by using the **sample** method:\n",
    "```Python\n",
    "    ### Samples k (batch size) experience tuples randomly from memory\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)```\n",
    "This simply draws a k number of experiences from the memory container (controlled by hyperparameter 'batch_size'), unpacks them into individual tensors so they can be fed to a neural network, and sends them to the training device. The batch sample is returned as a tuple of tensors for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "## Training the agent\n",
    "The 'ddpg' function trains the agent for a number of timesteps 'max_t' over a number of episodes 'n_episodes', saving checkpoints named 'output' for Actor and Critic networks if the environment has been solved:\n",
    "```Python\n",
    "def ddpg(output, n_episodes=1000, max_t=1000):```\n",
    "It keeps track of performance in the last 100 episodes which makes it easier to assess if the solving criteria has been reached, as well as score per episode, which is the only variable that it returns upon completion:\n",
    "```Python\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []```\n",
    "The function runs an episode and timestep loop with the correct syntax for Unity environments. 'max_t' is set to a default of 1,000 timesteps which is the episode length for Reacher.\n",
    "```Python\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]           # reset the environment \n",
    "        state = env_info.vector_observations[0]                     # get the current state\n",
    "        score = 0                                                   # initialize the score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)                               # select an action\n",
    "            env_info = env.step(action)[brain_name]                 # send action to tne environment\n",
    "            next_state = env_info.vector_observations[0]            # get next state\n",
    "            reward = env_info.rewards[0]                            # get reward\n",
    "            done = env_info.local_done[0]                           # check if episode finished\n",
    "            agent.step(state, action, reward, next_state, done)     # agent takes one step to train\n",
    "            state = next_state                                      # roll over state to next time step\n",
    "            score += reward                                         # update the score\n",
    "            if done:                                                # exit loop if episode finished\n",
    "                break```\n",
    "Scores are kept track of and printed for monitoring purposes and if the solving criteria of mean score > 30.0 over the latest 100 episodes (checked as mean of the 100-episode-long deque), the episode loop is broken and checkpoints saved for Actor and Critic networks, with the prefix defined by function argument 'output':\n",
    "```Python\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_online.state_dict(), '{}_checkpoint_actor.pth'.format(output))\n",
    "            torch.save(agent.critic_online.state_dict(), '{}_checkpoint_critic.pth'.format(output))\n",
    "            break```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "I will comment on the hyperparameters which I found most effective to experiment with.   \n",
    "\n",
    "**Buffer & Batch Size** In practise, I found a higher batch size (1024) seemed to work best in this environment. It has a relatively complex (33-d) and continuous state space, which suggests the variability between trajectories could be large. Therefore it makes sense that the agent might have to perform a large number of episodes in order to obtain performance representative of the particular policy it's currently under. With this in mind, given Reacher episodes are 1,000 timesteps, it was necessary to increase the buffer size to accomodate this elevated batch size.  \n",
    "```Python\n",
    "BUFFER_SIZE = int(3e6)     # replay buffer size\n",
    "BATCH_SIZE = 1024          # minibatch size```\n",
    "\n",
    "**Learning rates for Actor and Critic** I found a slightly higher learning rate for the critic tended to work best.  \n",
    "```Python\n",
    "LR_ACTOR = 1e-4            # learning rate of the actor \n",
    "LR_CRITIC = 2e-4           # learning rate of the critic```\n",
    "\n",
    "**Update every** In early attempts I updated the network more frequently. Similar to suggested in the exercise setup, this didn't work particularly well. In my winning submission, once a batch has been completed, the agent updates itself 3 times every 20 timesteps.\n",
    "```Python\n",
    "update_every = 20          # number of timesteps after which to run an update```\n",
    "\n",
    "**Starting Noise, Noise Decay and Noise Minimum** Since DDPG is a deterministic policy approach, we need to make sure the agent explores, especially at the beginning, in order to learn a good policy. In my implementation I do this by drawing noise from a normal distribution with mean 0 and standard deviation equal to 'noise scale', then adding this noise to the action output. I elected to choose a not too high value for the noise scale, to prevent the output getting clipped too often to -1 or 1, which would defeat the point of exploration. Throughout training, we would like the agent to reduce its exploratory actions. My strategy was to decrease the noise scale exponentially (by multiplying it every timestep by a decay constant 'noise decay') down to a noise floor of 'noise minimum'. This ensure the agent retained a certain degree of exploration throughout the whole training schedule. This may also help with the robustness of learnt representations and generalization.\n",
    "```Python\n",
    "SN = 0.25                  # starting value for additive noise scale (exploratory actions)\n",
    "ND = 0.99999               # noise decay rate (exploratory actions)\n",
    "NM = 0.01                  # noise minimum to be maintained (exploratory actions)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Submission results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solved the environment in 365 episodes, having achieved an\n",
    "average score of 30.04.  \n",
    "<img align=\"left\" src=\"Final_score.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "## Ideas for future improvement\n",
    "**Prioritizing Experiences** The random sampling of the Replay Buffer when training seems to work fine but I would improve upon it by prioritizing those experiences where the difference between online and target Q values was highest. This would involve some rewriting of the code, since the loss function would also be affected. The reason for this is that in a correct implementation of Prioritized Experience Replay, experiences need to be weighted in proportion to their likelihood of occurring, otherwise the agent may overfit to very informative but rare experiences. The idea of Prioritized Experience Replay appeals a lot to me due to its biological inspiration: in the brain, experiences are tagged by their emotional content and abundant evidence suggests replay in the mammalian brain during sleep is scaled by this parameter.  \n",
    "\n",
    "**Exploration Schedule** The agent's score is very low for the 200 first episodes, showing a steep and sudden improvement between 200 and 300, and another bump in improvement rate after episode 300. Below, I plotted the noise scale used in my schedule at each episode number for generating exploratory actions. There is an uncanny match between the stage in noise scale below and rate of score improvement above. The agent's performance in the first 200 episodes overlaps with a period of fairly high noise scale, which may be masking the agent's 'true potential' in the task ie, it is already *capable* of achieving a higher score its current actions suffer from having too much additive noise. Granted, it is this period of early exploration which *enables* the agent to navigate the task's high-dimensional parameter structure and eventually arrive at a good policy, but I wonder if alternative noise schedules (linear decrease, periodic such as a sine function with decreasing amplitude see below) would lead to faster learning. Explore/exploit dynamics in real-world biological agents are not monotonic however; many animals instead switch between 'perform' and 'play' modes, at least when the situation is not threatening to survival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.plot([max(0.01, 0.25*0.99999**t) for t in range(1000000)]);\n",
    "plt.xlim((0,500000));\n",
    "plt.xticks(range(0,500000,100000), labels=range(0,500,100));\n",
    "plt.xlabel('Episode', size=16);\n",
    "plt.ylabel('Noise scale', size=16);\n",
    "\n",
    "## Each episode is 1,000 timesteps therefore episode number = timestep/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Example periodic decreasing schedule\n",
    "plt.plot([1+math.sin(i)*-1/i for i in range(5,100) ]);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
